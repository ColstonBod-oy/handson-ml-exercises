{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Ohmls9bqAjjM",
      "metadata": {
        "id": "Ohmls9bqAjjM"
      },
      "source": [
        "**Chapter 2 â€“ End-to-end Machine Learning project**\n",
        "\n",
        "*Welcome to Machine Learning Housing Corp.! Your task is to predict median house values in Californian districts, given a number of features from these districts.*\n",
        "\n",
        "*This notebook contains all the sample code and solutions to the exercices in chapter 2.*\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ColstonBod-oy/handson-ml-exercises/blob/main/Housing.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dTqnO0dBAjkO",
      "metadata": {
        "id": "dTqnO0dBAjkO"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cAETnrXWC3UC",
      "metadata": {
        "id": "cAETnrXWC3UC"
      },
      "source": [
        "Using this chapter's housing dataset:\n",
        "\n",
        "\n",
        "1.   Try a Support Vector Machine regressor (`sklearn.svm.SVR`), with various hyperparameters such as `kernel=\"linear\"` (with various values for the `C` hyperparameter) or `kernel=\"rbf\"` (with various values for the `C` and `gamma` hyperparameters). Don't worry about what these hyperparameters mean for now. How does the best `SVR` predictor perform?\n",
        "\n",
        "2.   Try replacing `GridSearchCV` with `RandomizedSearchCV`.\n",
        "\n",
        "3.   Try adding a transformer in the preparation pipeline to select only the most important attributes.\n",
        "\n",
        "4.   Try creating a single pipeline that does the full data preparation plus the final prediction.\n",
        "\n",
        "5.   Automatically explore some preparation options using `GridSearchCV`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VMktCTX0AjjV",
      "metadata": {
        "id": "VMktCTX0AjjV"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "clJFVToSAjjV",
      "metadata": {
        "id": "clJFVToSAjjV"
      },
      "source": [
        "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "cJX4NnOtAjjW",
      "metadata": {
        "id": "cJX4NnOtAjjW"
      },
      "outputs": [],
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"end_to_end_project\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9_iNowFBAjjY",
      "metadata": {
        "id": "9_iNowFBAjjY"
      },
      "source": [
        "# Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "7EN72NCYAjja",
      "metadata": {
        "id": "7EN72NCYAjja"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ColstonBod-oy/handson-ml-exercises/main/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    os.makedirs(housing_path, exist_ok=True)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "rN1_pf-rAjjb",
      "metadata": {
        "id": "rN1_pf-rAjjb"
      },
      "outputs": [],
      "source": [
        "fetch_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "k9FZk71NAjjc",
      "metadata": {
        "id": "k9FZk71NAjjc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "yJgQyLgEAjjd",
      "metadata": {
        "id": "yJgQyLgEAjjd"
      },
      "outputs": [],
      "source": [
        "housing = load_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "697N5_sNAjjh",
      "metadata": {
        "id": "697N5_sNAjjh"
      },
      "outputs": [],
      "source": [
        "# to make this notebook's output identical at every run\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8erJMRRHU-I0",
      "metadata": {
        "id": "8erJMRRHU-I0"
      },
      "source": [
        "# Use stratified sampling method on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "K-snGUPsAjjl",
      "metadata": {
        "id": "K-snGUPsAjjl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FMXeiYYPAjjn",
      "metadata": {
        "id": "FMXeiYYPAjjn"
      },
      "source": [
        "**Warning**: in the book, I did not use `pd.cut()`. The `pd.cut()` solution gives the same result (except the labels are integers instead of floats), but it is simpler to understand:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "hGJsc3lPAjjn",
      "metadata": {
        "id": "hGJsc3lPAjjn"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "yooFEOdMAjjo",
      "metadata": {
        "id": "yooFEOdMAjjo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "OiO6_a7fAjjr",
      "metadata": {
        "id": "OiO6_a7fAjjr"
      },
      "outputs": [],
      "source": [
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WFhRO6aSBJyZ",
      "metadata": {
        "id": "WFhRO6aSBJyZ"
      },
      "source": [
        "# Prepare the data for Machine Learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "rZ5SWhKNBJyZ",
      "metadata": {
        "id": "rZ5SWhKNBJyZ"
      },
      "outputs": [],
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fuiWuB-FBJyc",
      "metadata": {
        "id": "fuiWuB-FBJyc"
      },
      "source": [
        "**Warning**: Since Scikit-Learn 0.20, the `sklearn.preprocessing.Imputer` class was replaced by the `sklearn.impute.SimpleImputer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "yIkEk3f9BJyc",
      "metadata": {
        "id": "yIkEk3f9BJyc"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\n",
        "except ImportError:\n",
        "    from sklearn.preprocessing import Imputer as SimpleImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Ued1PUXBJyd",
      "metadata": {
        "id": "0Ued1PUXBJyd"
      },
      "source": [
        "Remove the text attribute because median can only be calculated on numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "klqDCasgBJyd",
      "metadata": {
        "id": "klqDCasgBJyd"
      },
      "outputs": [],
      "source": [
        "housing_num = housing.drop('ocean_proximity', axis=1)\n",
        "# alternatively: housing_num = housing.select_dtypes(include=[np.number])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RDydQmybBJyh",
      "metadata": {
        "id": "RDydQmybBJyh"
      },
      "source": [
        "**Warning**: earlier versions of the book used the `LabelBinarizer` or `CategoricalEncoder` classes to convert each categorical value to a one-hot vector. It is now preferable to use the `OneHotEncoder` class. Since Scikit-Learn 0.20 it can handle string categorical inputs (see [PR #10521](https://github.com/scikit-learn/scikit-learn/issues/10521)), not just integer categorical inputs. If you are using an older version of Scikit-Learn, you can import the new version from `future_encoders.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "TlmVYqZ2BJyh",
      "metadata": {
        "id": "TlmVYqZ2BJyh"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "except ImportError:\n",
        "    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WAC6RLvhBJyj",
      "metadata": {
        "id": "WAC6RLvhBJyj"
      },
      "source": [
        "Let's create a custom transformer to add extra attributes:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H9jhHs4NBJyk",
      "metadata": {
        "id": "H9jhHs4NBJyk"
      },
      "source": [
        "You can use Scikit-Learn's `FunctionTransformer` class that lets you easily create a transformer based on a transformation function (thanks to [Hanmin Qin](https://github.com/qinhanmin2014) for suggesting this code). Note that we need to set `validate=False` because the data contains non-float values (`validate` will default to `False` in Scikit-Learn 0.22)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "7B4aT4agBJyk",
      "metadata": {
        "id": "7B4aT4agBJyk"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\n",
        "rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
        "    list(housing.columns).index(col)\n",
        "    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n",
        "\n",
        "def add_extra_features(X, add_bedrooms_per_room=True):\n",
        "    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
        "    population_per_household = X[:, population_ix] / X[:, household_ix]\n",
        "    if add_bedrooms_per_room:\n",
        "        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "        return np.c_[X, rooms_per_household, population_per_household,\n",
        "                     bedrooms_per_room]\n",
        "    else:\n",
        "        return np.c_[X, rooms_per_household, population_per_household]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J3UndQasBJyk",
      "metadata": {
        "id": "J3UndQasBJyk"
      },
      "source": [
        "Now let's build a pipeline for preprocessing the numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "DHrc2EmaBJyl",
      "metadata": {
        "id": "DHrc2EmaBJyl"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Td0ZsxRgBJyl",
      "metadata": {
        "id": "Td0ZsxRgBJyl"
      },
      "source": [
        "**Warning**: earlier versions of the book applied different transformations to different columns using a solution based on a `DataFrameSelector` transformer and a `FeatureUnion`. It is now preferable to use the `ColumnTransformer` class that was introduced in Scikit-Learn 0.20. If you are using an older version of Scikit-Learn, you can import it from `future_encoders.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "Y4DNJEs1BJyl",
      "metadata": {
        "id": "Y4DNJEs1BJyl"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "except ImportError:\n",
        "    from future_encoders import ColumnTransformer # Scikit-Learn < 0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "Jhe99_QGBJym",
      "metadata": {
        "id": "Jhe99_QGBJym"
      },
      "outputs": [],
      "source": [
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeAzw8f8ygdN"
      },
      "source": [
        "# Select and fine-tune a model "
      ],
      "id": "KeAzw8f8ygdN"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "P-7eudO-ygdV"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "id": "P-7eudO-ygdV"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "JKmTddkkygdX",
        "outputId": "cab58257-fadd-46f2-9730-efe426ee4467",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n",
              "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
              "                          'n_estimators': [3, 10, 30]},\n",
              "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
              "                          'n_estimators': [3, 10]}],\n",
              "             return_train_score=True, scoring='neg_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    # try 12 (3Ã—4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2Ã—3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ],
      "id": "JKmTddkkygdX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JPZ-RO1jEle"
      },
      "source": [
        "The best model achieves the following score (evaluated using 5-fold cross validation):"
      ],
      "id": "0JPZ-RO1jEle"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176188b9-bf42-4c71-b9df-ef2251a3205e",
        "id": "LhOkc9DvjLjL"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49898.98913455217"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "negative_mse = grid_search.best_score_\n",
        "rmse = np.sqrt(-negative_mse)\n",
        "rmse"
      ],
      "id": "LhOkc9DvjLjL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp1oehEyvk4D"
      },
      "source": [
        "The best hyperparameter combination found:"
      ],
      "id": "Wp1oehEyvk4D"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "4HjaF5r4vk4D",
        "outputId": "061c07fd-f649-4f44-d697-39b949076809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_features': 8, 'n_estimators': 30}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "grid_search.best_params_"
      ],
      "id": "4HjaF5r4vk4D"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "4hqzy6fOvk4E",
        "outputId": "3e74b3fb-9e9d-4549-fb3a-a183c9773498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(max_features=8, n_estimators=30, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "grid_search.best_estimator_"
      ],
      "id": "4hqzy6fOvk4E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze the best models and their errors"
      ],
      "metadata": {
        "id": "bCTlfbf4BASE"
      },
      "id": "bCTlfbf4BASE"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "aih9_1XOygda",
        "outputId": "e52171cd-881d-4d5b-c1b9-94ab3d42e2c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6.96542523e-02, 6.04213840e-02, 4.21882202e-02, 1.52450557e-02,\n",
              "       1.55545295e-02, 1.58491147e-02, 1.49346552e-02, 3.79009225e-01,\n",
              "       5.47789150e-02, 1.07031322e-01, 4.82031213e-02, 6.79266007e-03,\n",
              "       1.65706303e-01, 7.83480660e-05, 1.52473276e-03, 3.02816106e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "feature_importances"
      ],
      "id": "aih9_1XOygda"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gvsqocpUygda",
        "outputId": "cbe5b055-91f6-4235-e99c-c5df43951994",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3790092248170967, 'median_income'),\n",
              " (0.16570630316895876, 'INLAND'),\n",
              " (0.10703132208204354, 'pop_per_hhold'),\n",
              " (0.06965425227942929, 'longitude'),\n",
              " (0.0604213840080722, 'latitude'),\n",
              " (0.054778915018283726, 'rooms_per_hhold'),\n",
              " (0.048203121338269206, 'bedrooms_per_room'),\n",
              " (0.04218822024391753, 'housing_median_age'),\n",
              " (0.015849114744428634, 'population'),\n",
              " (0.015554529490469328, 'total_bedrooms'),\n",
              " (0.01524505568840977, 'total_rooms'),\n",
              " (0.014934655161887776, 'households'),\n",
              " (0.006792660074259966, '<1H OCEAN'),\n",
              " (0.0030281610628962747, 'NEAR OCEAN'),\n",
              " (0.0015247327555504937, 'NEAR BAY'),\n",
              " (7.834806602687504e-05, 'ISLAND')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
        "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
        "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
        "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
        "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
        "sorted(zip(feature_importances, attributes), reverse=True)"
      ],
      "id": "gvsqocpUygda"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate your system on the test set"
      ],
      "metadata": {
        "id": "t5jW2z_aCSST"
      },
      "id": "t5jW2z_aCSST"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "WCU9qAzNygdR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ],
      "id": "WCU9qAzNygdR"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "4oiRHPTnygdb"
      },
      "outputs": [],
      "source": [
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)"
      ],
      "id": "4oiRHPTnygdb"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "DGsutdZiygdb",
        "outputId": "ea299db9-508e-4581-9f75-fccdd46dc7c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47873.26095812988"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "final_rmse"
      ],
      "id": "DGsutdZiygdb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfTbYuiJygdb"
      },
      "source": [
        "We can compute a 95% confidence interval for the test RMSE:"
      ],
      "id": "hfTbYuiJygdb"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "dcJqeVj-ygdb"
      },
      "outputs": [],
      "source": [
        "from scipy import stats"
      ],
      "id": "dcJqeVj-ygdb"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "j2ikmJMAygdc",
        "outputId": "5300125a-3208-410b-bf18-262228b263c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([45893.36082829, 49774.46796717])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "confidence = 0.95\n",
        "squared_errors = (final_predictions - y_test) ** 2\n",
        "mean = squared_errors.mean()\n",
        "m = len(squared_errors)\n",
        "\n",
        "np.sqrt(stats.t.interval(confidence, m - 1,\n",
        "                         loc=np.mean(squared_errors),\n",
        "                         scale=stats.sem(squared_errors)))"
      ],
      "id": "j2ikmJMAygdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kqea6Q6ygdc"
      },
      "source": [
        "We could compute the interval manually like this:"
      ],
      "id": "4kqea6Q6ygdc"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "CJtjVb9Zygdc",
        "outputId": "2fac5931-9b82-4c1b-d9c0-f829acfd5df0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45893.360828285535, 49774.46796717361)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "tscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\n",
        "tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\n",
        "np.sqrt(mean - tmargin), np.sqrt(mean + tmargin)"
      ],
      "id": "CJtjVb9Zygdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OADAayrzygdd"
      },
      "source": [
        "Alternatively, we could use a z-scores rather than t-scores:"
      ],
      "id": "OADAayrzygdd"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "VRYFDrZCygdd",
        "outputId": "5a76149a-3502-4240-80e6-72e6a9c1b9da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45893.9540110131, 49773.921030650374)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "zscore = stats.norm.ppf((1 + confidence) / 2)\n",
        "zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\n",
        "np.sqrt(mean - zmargin), np.sqrt(mean + zmargin)"
      ],
      "id": "VRYFDrZCygdd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf3Kgq4V7Jlg"
      },
      "source": [
        "# Exercise solutions"
      ],
      "id": "wf3Kgq4V7Jlg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDgWcqOj7Jlh"
      },
      "source": [
        "## 1."
      ],
      "id": "WDgWcqOj7Jlh"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "p4354y_q7Jli"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR"
      ],
      "id": "p4354y_q7Jli"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "outputId": "7f396ff2-0f18-4f51-f812-9cd01ab17b45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFaylRsQ7Jli"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=SVR(), n_jobs=4,\n",
              "             param_grid=[{'C': [10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0,\n",
              "                                10000.0, 30000.0],\n",
              "                          'kernel': ['linear']},\n",
              "                         {'C': [1.0, 3.0, 10.0, 30.0, 100.0, 300.0, 1000.0],\n",
              "                          'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n",
              "                          'kernel': ['rbf']}],\n",
              "             scoring='neg_mean_squared_error', verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
        "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
        "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
        "    ]\n",
        "\n",
        "svm_reg = SVR()\n",
        "grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ],
      "id": "nFaylRsQ7Jli"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZUCVfW87Jlj"
      },
      "source": [
        "The best model achieves the following score (evaluated using 5-fold cross validation):"
      ],
      "id": "CZUCVfW87Jlj"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "YtlJFx3r7Jlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8648d1-11f6-4ab6-9015-df4c4201df0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70286.61835383571"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "negative_mse = grid_search.best_score_\n",
        "rmse = np.sqrt(-negative_mse)\n",
        "rmse"
      ],
      "id": "YtlJFx3r7Jlk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7r7ma1P7Jlk"
      },
      "source": [
        "That's much worse than the `RandomForestRegressor`. Let's check the best hyperparameters found:"
      ],
      "id": "w7r7ma1P7Jlk"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "3OzDrgyn7Jll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9687b53e-c60a-45de-b7a3-4ff404f794bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 30000.0, 'kernel': 'linear'}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "grid_search.best_params_"
      ],
      "id": "3OzDrgyn7Jll"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1majvwa7Jll"
      },
      "source": [
        "The linear kernel seems better than the RBF kernel. Notice that the value of `C` is the maximum tested value. When this happens you definitely want to launch the grid search again with higher values for `C` (removing the smallest values), because it is likely that higher values of `C` will be better."
      ],
      "id": "V1majvwa7Jll"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLgI7FJZvk4M"
      },
      "source": [
        "## 2."
      ],
      "id": "gLgI7FJZvk4M"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "dua7VmTOvk4M",
        "outputId": "553b5532-0c75-4ab0-bcab-a533b5b22786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, estimator=SVR(), n_iter=50, n_jobs=4,\n",
              "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f1d9947bf50>,\n",
              "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f1d9950b4d0>,\n",
              "                                        'kernel': ['linear', 'rbf']},\n",
              "                   random_state=42, scoring='neg_mean_squared_error',\n",
              "                   verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import expon, reciprocal\n",
        "\n",
        "# see https://docs.scipy.org/doc/scipy/reference/stats.html\n",
        "# for `expon()` and `reciprocal()` documentation and more probability distribution functions.\n",
        "\n",
        "# Note: gamma is ignored when kernel is \"linear\"\n",
        "param_distribs = {\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'C': reciprocal(20, 200000),\n",
        "        'gamma': expon(scale=1.0),\n",
        "    }\n",
        "\n",
        "svm_reg = SVR()\n",
        "rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n",
        "                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n",
        "                                verbose=2, n_jobs=4, random_state=42)\n",
        "rnd_search.fit(housing_prepared, housing_labels)"
      ],
      "id": "dua7VmTOvk4M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6PH-2Avk4M"
      },
      "source": [
        "The best model achieves the following score (evaluated using 5-fold cross validation):"
      ],
      "id": "vV6PH-2Avk4M"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "mgvRiu1Gvk4M",
        "outputId": "0c737990-be3c-4cdb-a3ab-a44e410157ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54751.69009488048"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "negative_mse = rnd_search.best_score_\n",
        "rmse = np.sqrt(-negative_mse)\n",
        "rmse"
      ],
      "id": "mgvRiu1Gvk4M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ojDRJqvk4N"
      },
      "source": [
        "Now this is much closer to the performance of the `RandomForestRegressor` (but not quite there yet). Let's check the best hyperparameters found:"
      ],
      "id": "F2ojDRJqvk4N"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "SbWNQ3olvk4N",
        "outputId": "2d3afadb-82e9-4dbf-cc4e-830beaf2ca6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 157055.10989448498, 'gamma': 0.26497040005002437, 'kernel': 'rbf'}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "rnd_search.best_params_"
      ],
      "id": "SbWNQ3olvk4N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4yoYSllvk4N"
      },
      "source": [
        "This time the search found a good set of hyperparameters for the RBF kernel. Randomized search tends to find better hyperparameters than grid search in the same amount of time."
      ],
      "id": "R4yoYSllvk4N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL4KLlDpvk4O"
      },
      "source": [
        "The reciprocal distribution is useful when you have no idea what the scale of the hyperparameter should be (all scales are equally likely, within the given range), whereas the exponential distribution is best when you know (more or less) what the scale of the hyperparameter should be."
      ],
      "id": "NL4KLlDpvk4O"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}