{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Ohmls9bqAjjM",
      "metadata": {
        "id": "Ohmls9bqAjjM"
      },
      "source": [
        "**Chapter 2 – End-to-end Machine Learning project**\n",
        "\n",
        "*Welcome to Machine Learning Housing Corp.! Your task is to predict median house values in Californian districts, given a number of features from these districts.*\n",
        "\n",
        "*This notebook contains all the sample code and solutions to the exercices in chapter 2.*\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ColstonBod-oy/handson-ml-exercises/blob/main/Housing.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dTqnO0dBAjkO",
      "metadata": {
        "id": "dTqnO0dBAjkO"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cAETnrXWC3UC",
      "metadata": {
        "id": "cAETnrXWC3UC"
      },
      "source": [
        "Using this chapter's housing dataset:\n",
        "\n",
        "\n",
        "1.   Try a Support Vector Machine regressor (`sklearn.svm.SVR`), with various hyperparameters such as `kernel=\"linear\"` (with various values for the `C` hyperparameter) or `kernel=\"rbf\"` (with various values for the `C` and `gamma` hyperparameters). Don't worry about what these hyperparameters mean for now. How does the best `SVR` predictor perform?\n",
        "\n",
        "2.   Try replacing `GridSearchCV` with `RandomizedSearchCV`.\n",
        "\n",
        "3.   Try adding a transformer in the preparation pipeline to select only the most important attributes.\n",
        "\n",
        "4.   Try creating a single pipeline that does the full data preparation plus the final prediction.\n",
        "\n",
        "5.   Automatically explore some preparation options using `GridSearchCV`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VMktCTX0AjjV",
      "metadata": {
        "id": "VMktCTX0AjjV"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "clJFVToSAjjV",
      "metadata": {
        "id": "clJFVToSAjjV"
      },
      "source": [
        "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cJX4NnOtAjjW",
      "metadata": {
        "id": "cJX4NnOtAjjW"
      },
      "outputs": [],
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"end_to_end_project\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9_iNowFBAjjY",
      "metadata": {
        "id": "9_iNowFBAjjY"
      },
      "source": [
        "# Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7EN72NCYAjja",
      "metadata": {
        "id": "7EN72NCYAjja"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ColstonBod-oy/handson-ml-exercises/main/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    os.makedirs(housing_path, exist_ok=True)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "rN1_pf-rAjjb",
      "metadata": {
        "id": "rN1_pf-rAjjb"
      },
      "outputs": [],
      "source": [
        "fetch_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "k9FZk71NAjjc",
      "metadata": {
        "id": "k9FZk71NAjjc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "yJgQyLgEAjjd",
      "metadata": {
        "id": "yJgQyLgEAjjd"
      },
      "outputs": [],
      "source": [
        "housing = load_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "697N5_sNAjjh",
      "metadata": {
        "id": "697N5_sNAjjh"
      },
      "outputs": [],
      "source": [
        "# to make this notebook's output identical at every run\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8erJMRRHU-I0",
      "metadata": {
        "id": "8erJMRRHU-I0"
      },
      "source": [
        "# Use stratified sampling method on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "K-snGUPsAjjl",
      "metadata": {
        "id": "K-snGUPsAjjl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FMXeiYYPAjjn",
      "metadata": {
        "id": "FMXeiYYPAjjn"
      },
      "source": [
        "**Warning**: in the book, I did not use `pd.cut()`. The `pd.cut()` solution gives the same result (except the labels are integers instead of floats), but it is simpler to understand:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "hGJsc3lPAjjn",
      "metadata": {
        "id": "hGJsc3lPAjjn"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "yooFEOdMAjjo",
      "metadata": {
        "id": "yooFEOdMAjjo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "OiO6_a7fAjjr",
      "metadata": {
        "id": "OiO6_a7fAjjr"
      },
      "outputs": [],
      "source": [
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WFhRO6aSBJyZ",
      "metadata": {
        "id": "WFhRO6aSBJyZ"
      },
      "source": [
        "# Prepare the data for Machine Learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "rZ5SWhKNBJyZ",
      "metadata": {
        "id": "rZ5SWhKNBJyZ"
      },
      "outputs": [],
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fuiWuB-FBJyc",
      "metadata": {
        "id": "fuiWuB-FBJyc"
      },
      "source": [
        "**Warning**: Since Scikit-Learn 0.20, the `sklearn.preprocessing.Imputer` class was replaced by the `sklearn.impute.SimpleImputer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "yIkEk3f9BJyc",
      "metadata": {
        "id": "yIkEk3f9BJyc"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\n",
        "except ImportError:\n",
        "    from sklearn.preprocessing import Imputer as SimpleImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Ued1PUXBJyd",
      "metadata": {
        "id": "0Ued1PUXBJyd"
      },
      "source": [
        "Remove the text attribute because median can only be calculated on numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "klqDCasgBJyd",
      "metadata": {
        "id": "klqDCasgBJyd"
      },
      "outputs": [],
      "source": [
        "housing_num = housing.drop('ocean_proximity', axis=1)\n",
        "# alternatively: housing_num = housing.select_dtypes(include=[np.number])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RDydQmybBJyh",
      "metadata": {
        "id": "RDydQmybBJyh"
      },
      "source": [
        "**Warning**: earlier versions of the book used the `LabelBinarizer` or `CategoricalEncoder` classes to convert each categorical value to a one-hot vector. It is now preferable to use the `OneHotEncoder` class. Since Scikit-Learn 0.20 it can handle string categorical inputs (see [PR #10521](https://github.com/scikit-learn/scikit-learn/issues/10521)), not just integer categorical inputs. If you are using an older version of Scikit-Learn, you can import the new version from `future_encoders.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "TlmVYqZ2BJyh",
      "metadata": {
        "id": "TlmVYqZ2BJyh"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "except ImportError:\n",
        "    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WAC6RLvhBJyj",
      "metadata": {
        "id": "WAC6RLvhBJyj"
      },
      "source": [
        "Let's create a custom transformer to add extra attributes:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H9jhHs4NBJyk",
      "metadata": {
        "id": "H9jhHs4NBJyk"
      },
      "source": [
        "You can use Scikit-Learn's `FunctionTransformer` class that lets you easily create a transformer based on a transformation function (thanks to [Hanmin Qin](https://github.com/qinhanmin2014) for suggesting this code). Note that we need to set `validate=False` because the data contains non-float values (`validate` will default to `False` in Scikit-Learn 0.22)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7B4aT4agBJyk",
      "metadata": {
        "id": "7B4aT4agBJyk"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\n",
        "rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
        "    list(housing.columns).index(col)\n",
        "    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n",
        "\n",
        "def add_extra_features(X, add_bedrooms_per_room=True):\n",
        "    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
        "    population_per_household = X[:, population_ix] / X[:, household_ix]\n",
        "    if add_bedrooms_per_room:\n",
        "        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "        return np.c_[X, rooms_per_household, population_per_household,\n",
        "                     bedrooms_per_room]\n",
        "    else:\n",
        "        return np.c_[X, rooms_per_household, population_per_household]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J3UndQasBJyk",
      "metadata": {
        "id": "J3UndQasBJyk"
      },
      "source": [
        "Now let's build a pipeline for preprocessing the numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "DHrc2EmaBJyl",
      "metadata": {
        "id": "DHrc2EmaBJyl"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Td0ZsxRgBJyl",
      "metadata": {
        "id": "Td0ZsxRgBJyl"
      },
      "source": [
        "**Warning**: earlier versions of the book applied different transformations to different columns using a solution based on a `DataFrameSelector` transformer and a `FeatureUnion`. It is now preferable to use the `ColumnTransformer` class that was introduced in Scikit-Learn 0.20. If you are using an older version of Scikit-Learn, you can import it from `future_encoders.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Y4DNJEs1BJyl",
      "metadata": {
        "id": "Y4DNJEs1BJyl"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "except ImportError:\n",
        "    from future_encoders import ColumnTransformer # Scikit-Learn < 0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Jhe99_QGBJym",
      "metadata": {
        "id": "Jhe99_QGBJym"
      },
      "outputs": [],
      "source": [
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeAzw8f8ygdN"
      },
      "source": [
        "# Select and fine-tune a model "
      ],
      "id": "KeAzw8f8ygdN"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "P-7eudO-ygdV"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "id": "P-7eudO-ygdV"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JKmTddkkygdX",
        "outputId": "341cbac0-59b0-4d93-e117-6d1d767a9d0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n",
              "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
              "                          'n_estimators': [3, 10, 30]},\n",
              "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
              "                          'n_estimators': [3, 10]}],\n",
              "             return_train_score=True, scoring='neg_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    # try 12 (3×4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2×3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ],
      "id": "JKmTddkkygdX"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}